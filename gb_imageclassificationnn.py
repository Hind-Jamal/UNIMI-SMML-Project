# -*- coding: utf-8 -*-
"""GB-ImageClassificationNN.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1CwZMCIw8YKWtc8nDbXkoTonHR5tpFT6u

# Image Classification with Neural Networks

Statistical Methods for Machine Learning - AA 2019/2020

Universit√† degli Studi di Milano

Gerard Baholli 943594

## Setup
"""

import numpy as np
import os
import PIL
import PIL.Image
import tensorflow as tf
import tensorflow_datasets as tfds

print(tf.__version__)

import pathlib

data_dir = pathlib.Path("/content/drive/My Drive/Colab Notebooks/dataset2")
image_count = len(list(data_dir.glob('*/*.jpg')))
print(image_count)

"""## Preprocessing

### Create a dataset
"""

batch_size = 64
img_height = 32
img_width = 32

train_ds = tf.keras.preprocessing.image_dataset_from_directory(
  data_dir,
  validation_split=0.2,
  subset="training",
  seed=123,
  image_size=(img_height, img_width),
  batch_size=batch_size
  )

val_ds = tf.keras.preprocessing.image_dataset_from_directory(
  data_dir,
  validation_split=0.2,
  subset="validation",
  seed=123,
  image_size=(img_height, img_width),
  batch_size=batch_size
  )

class_names = train_ds.class_names
print(class_names)

"""### Configure the dataset for performance

Let's make sure to use buffered prefetching so we can yield data from disk without having I/O become blocking.
"""

AUTOTUNE = tf.data.experimental.AUTOTUNE

train_ds = train_ds.cache().prefetch(buffer_size=AUTOTUNE)
val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)

"""## Defs"""

from tensorflow.keras import layers
import matplotlib.pyplot as plt

num_classes = 10
n_epochs = 20

# PLOT FUNCTION OF THE MODEL
def plt_history(history):

  acc = history.history['accuracy']
  val_acc = history.history['val_accuracy']

  loss = history.history['loss']
  val_loss = history.history['val_loss']

  epochs_range = range(n_epochs)

  plt.figure(figsize=(12, 8))
  plt.subplot(1, 2, 1)
  plt.plot(epochs_range, acc, label='Training Accuracy')
  plt.plot(epochs_range, val_acc, label='Validation Accuracy')
  plt.legend(loc='lower right')
  plt.title('Accuracy')

  plt.subplot(1, 2, 2)
  plt.plot(epochs_range, loss, label='Training Loss')
  plt.plot(epochs_range, val_loss, label='Validation Loss')
  plt.legend(loc='upper right')
  plt.title('Loss')
  return plt.show()

"""## Model 1

### Training 1
"""

model = tf.keras.Sequential([
  layers.experimental.preprocessing.Rescaling(1./255, input_shape=(img_height, img_width, 3)),
  layers.Flatten(),
  layers.Dense(15, activation='relu'),
  layers.Dense(num_classes)
])

model.compile(
  optimizer='adam',
  loss=tf.losses.SparseCategoricalCrossentropy(from_logits=True),
  metrics=['accuracy'])

history = model.fit(
  train_ds,
  validation_data = val_ds,
  epochs = n_epochs
)

model.summary()

plt_history(history)

evaluation = model.evaluate(val_ds)

print("Loss (zero-one-loss): ", 1-evaluation[1])

"""### Training 2"""

history = model.fit(
  train_ds,
  validation_data=val_ds,
  epochs = n_epochs
)

evaluation = model.evaluate(val_ds)

print("Loss (zero-one-loss): ", 1-evaluation[1])

"""## Model 2

### Training 1
"""

model2 = tf.keras.Sequential([
  layers.experimental.preprocessing.Rescaling(1./255, input_shape=(img_height, img_width, 3)),
  layers.Conv2D(1, kernel_size=(5, 5), padding='same', activation='relu'),
  layers.MaxPooling2D(),
  layers.Flatten(),
  layers.Dense(15, activation='relu'),
  layers.Dense(num_classes)
])

model2.compile(
  optimizer='adam',
  loss=tf.losses.SparseCategoricalCrossentropy(from_logits=True),
  metrics=['accuracy'])

history2 = model2.fit(
  train_ds,
  validation_data = val_ds,
  epochs = n_epochs
)

model2.summary()

plt_history(history2)

"""Evaluation 1:"""

evaluation2 = model2.evaluate(val_ds)

print("Loss (zero-one-loss): ", 1-evaluation2[1])

"""### Training 2"""

history2 = model2.fit(
  train_ds,
  validation_data=val_ds,
  epochs = n_epochs
)

"""Evaluation 2:"""

evaluation2 = model2.evaluate(val_ds)

print("Loss (zero-one-loss): ", 1-evaluation2[1])

"""## Model 3

### Training 1
"""

model3 = tf.keras.Sequential([
  layers.experimental.preprocessing.Rescaling(1./255, input_shape=(img_height, img_width, 3)),
  layers.Conv2D(1, kernel_size=(7, 7), padding='same', activation='relu'),
  layers.MaxPooling2D(),
  layers.Flatten(),
  layers.Dense(15, activation='relu'),
  layers.Dense(num_classes)
])

model3.compile(
  optimizer='adam',
  loss=tf.losses.SparseCategoricalCrossentropy(from_logits=True),
  metrics=['accuracy'])

history3 = model3.fit(
  train_ds,
  validation_data = val_ds,
  epochs = n_epochs
)

model3.summary()

plt_history(history3)

evaluation3 = model3.evaluate(val_ds)

print("Loss (zero-one-loss): ", 1-evaluation3[1])

"""### Training 2"""

history3 = model3.fit(
  train_ds,
  validation_data = val_ds,
  epochs = n_epochs
)

evaluation3 = model3.evaluate(val_ds)

print("Loss (zero-one-loss): ", 1-evaluation3[1])

"""## Model 4

### Training 1
"""

model4 = tf.keras.Sequential([
  layers.experimental.preprocessing.Rescaling(1./255, input_shape=(img_height, img_width, 3)),
  layers.Conv2D(3, kernel_size=(3, 3), padding='same', activation='relu'),
  layers.MaxPooling2D(),
  layers.Flatten(),
  layers.Dense(15, activation='relu'),
  layers.Dense(num_classes)
])

model4.compile(
  optimizer='adam',
  loss=tf.losses.SparseCategoricalCrossentropy(from_logits=True),
  metrics=['accuracy'])

history4 = model4.fit(
  train_ds,
  validation_data = val_ds,
  epochs = n_epochs
)

model4.summary()

plt_history(history4)

"""Evaluation 1:"""

evaluation4 = model4.evaluate(val_ds)

print("Loss (zero-one-loss): ", 1-evaluation4[1])

"""### Training 2"""

history4 = model4.fit(
  train_ds,
  validation_data=val_ds,
  epochs = n_epochs
)

"""Evaluation 2:"""

evaluation4 = model4.evaluate(val_ds)

print("Loss (zero-one-loss): ", 1-evaluation4[1])

"""## Model 5

### Training 1
"""

model5 = tf.keras.Sequential([
  layers.experimental.preprocessing.Rescaling(1./255, input_shape=(img_height, img_width, 3)),
  layers.Flatten(),
  layers.Dense(15, activation = 'relu' ),
  layers.Dense(10, activation = 'relu' ),
  layers.Dense(num_classes)
])

model5.compile(
    optimizer = 'sgd',
    loss=tf.losses.SparseCategoricalCrossentropy(from_logits=True),
    metrics = ['accuracy'])

history5 = model5.fit(
  train_ds,
  validation_data = val_ds,
  epochs = n_epochs
)

model5.summary()

plt_history(history5)

evaluation5 = model5.evaluate(val_ds)

print("Loss (zero-one-loss): ", 1-evaluation5[1])

"""### Training 2"""

history5 = model5.fit(
  train_ds,
  validation_data=val_ds,
  epochs = n_epochs
)

evaluation5 = model5.evaluate(val_ds)

print("Loss (zero-one-loss): ", 1-evaluation5[1])

"""## RESULTS"""

print("Model1: ", evaluation[1]*100)
print("Model2: ", evaluation2[1]*100)
print("Model3: ", evaluation3[1]*100)
print("Model4: ", evaluation4[1]*100)
print("Model5: ", evaluation5[1]*100)

"""## Finer control"""

list_ds = tf.data.Dataset.list_files(str(data_dir/'*/*'), shuffle=False)
list_ds = list_ds.shuffle(image_count, reshuffle_each_iteration=False)

val_size = int(image_count * 0.2)
train_ds = list_ds.skip(val_size)
val_ds = list_ds.take(val_size)

print(tf.data.experimental.cardinality(train_ds).numpy())
print(tf.data.experimental.cardinality(val_ds).numpy())

def get_label(file_path):
  # convert the path to a list of path components
  parts = tf.strings.split(file_path, os.path.sep)
  # The second to last is the class-directory
  one_hot = parts[-2] == class_names
  # Integer encode the label
  return tf.argmax(one_hot)

def decode_img(img):
  # convert the compressed string to a 3D uint8 tensor
  img = tf.image.decode_jpeg(img, channels=3)
  # resize the image to the desired size
  return tf.image.resize(img, [img_height, img_width])

def process_path(file_path):
  label = get_label(file_path)
  # load the raw data from the file as a string
  img = tf.io.read_file(file_path)
  img = decode_img(img)
  return img, label

# Set `num_parallel_calls` so multiple images are loaded/processed in parallel.
train_ds = train_ds.map(process_path, num_parallel_calls=AUTOTUNE)
val_ds = val_ds.map(process_path, num_parallel_calls=AUTOTUNE)

"""###  Configure dataset for performance"""

def configure_for_performance(ds):
  ds = ds.cache()
  ds = ds.shuffle(buffer_size=1000)
  ds = ds.batch(batch_size)
  ds = ds.prefetch(buffer_size=AUTOTUNE)
  return ds

train_ds = configure_for_performance(train_ds)
val_ds = configure_for_performance(val_ds)

"""### Random prediction"""

probability_model = tf.keras.Sequential([model, tf.keras.layers.Softmax()])

predictions = probability_model.predict(val_ds)

i_pred = 1

print(100*np.max(predictions[i_pred]), class_names[np.argmax(predictions[i_pred])])

"""### Choosen prediction"""

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers

image_size = (img_height, img_width)
batch_size = 32

img = keras.preprocessing.image.load_img(
    pathlib.Path("/content/drive/My Drive/Colab Notebooks/test/17_100.jpg"),
    target_size=image_size
)

plt.imshow(img)
img_array = keras.preprocessing.image.img_to_array(img)
img_array = tf.expand_dims(img_array, 0)  # Create batch axis

predictions = probability_model.predict(img_array)
score = predictions[0]

for i in range(10):
  print(str(int(100*score[i])) + "%", class_names[i])